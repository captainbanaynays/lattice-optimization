Crossover method is a well-established method of generating mutations - doesn't seem like it would be terribly efficient?

[https://www.mdpi.com/2227-7390/10/22/4364?type=check_update&version=2#B2-mathematics-10-04364](https://www.mdpi.com/2227-7390/10/22/4364?type=check_update&version=2#B2-mathematics-10-04364) 

[Wikipedia article on bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) 

[Benchmarking Stochastic Algorithms for Global Optimization Problems by Visualizing Confidence Intervals](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845658) 

> However, if we are not ready to make such a justification, then we can use the bootstrap instead. Using case resampling, we can derive the distribution of $\bar {x}$. We first resample the data to obtain a bootstrap resample. An example of the first resample might look like this $X_1^* = x_2, x_1, x_{10}, x_{10}, x_3, x_4, x_6, x_7, x_1, x_9$. There are some duplicates since a bootstrap resample comes from sampling with replacement from the data. Also the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first bootstrap mean: $\mu_1^*$ . We repeat this process to obtain the second resample $X_2^*$ and compute the second bootstrap mean $μ_2^*$ . If we repeat this 100 times, then we have $μ_1^*, μ_2^*, ..., μ_{100}^*$ . This represents an empirical bootstrap distribution of sample mean. From this empirical distribution, one can derive a bootstrap confidence interval for the purpose of hypothesis testing.

Okay so it sounds like we'll want to run the evolutionary algorithm as much as we possibly can to get a large sample of final minimal energy configurations, then resample to generate...hmm

Okay, so the first generation will always be completely random and is a good sample of the entire distribution. By examining the best config(s) from the last generation / the minima of the entire run, we can get a good idea of the final compared to the overall distribution. 

Can save just the energy state results from all generations / runs to add to a continuously growing sample distribution, compare best of each run of the algorithm to it each time and get an average standard deviation measure of the configs that are generated at the end. 

No way to give a level of confidence that we are seeing the actual global minima? We can only say that it's really really low in comparison to the rest of the configurations?

Also! We should transfer over the VASP data of the retained bottom %ages from the last generation so that we can avoid processing it again and getting the same results. 

[Evolutionary Algorithims](https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/Evolutionary%20Algorithms%20%5BKita%202011%5D.pdf)

Contains a lot on different evolutionary algorithims and their differences. Look at Quantum Evolutionary Algorithim? section 7.3.1

For some samples just do Ag/Bi swaps, for some do all 3
Genetic algorithim, crossover

For justification that we've found the global minimum, look at band gaps, it should align with the expirimental energy

Also look at XRD graph (spikes)

Ask about Gamma calculations in VASP (?)

Watch this eventually: [VASP Intro Lecture](https://youtu.be/Fv3F4LHGPuc) 

Look at `pymatgen`

Prepare 2-3 slides
Khare 10-1 and 3-6
